{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import torch\n", "import torch.nn as nn\n", "import torch.optim as optim\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n", "                           confusion_matrix, roc_curve, auc, precision_recall_curve, \n", "                           average_precision_score)\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.svm import SVC\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.neural_network import MLPClassifier\n", "import xgboost as xgb\n", "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n", "from imblearn.over_sampling import SMOTE\n", "from imblearn.under_sampling import RandomUnderSampler\n", "from collections import Counter\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "import datetime  # Add this import at the beginning of your script\n", "import warnings\n", "warnings.filterwarnings('ignore')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def visualize_spectra(normal_spectra, tumor_spectra, wavenumbers):\n", "    \"\"\"Visualize average spectra and their differences\"\"\"\n", "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 12))\n", "    \n", "    # Plot average spectra\n", "    normal_mean = np.mean(normal_spectra, axis=0)\n", "    normal_std = np.std(normal_spectra, axis=0)\n", "    tumor_mean = np.mean(tumor_spectra, axis=0)\n", "    tumor_std = np.std(tumor_spectra, axis=0)\n", "    \n", "    # Average spectra with standard deviation\n", "    ax1.plot(wavenumbers, normal_mean, 'b-', label='Normal Tissue', alpha=0.7)\n", "    ax1.fill_between(wavenumbers, normal_mean - normal_std, normal_mean + normal_std, \n", "                     color='b', alpha=0.2)\n", "    ax1.plot(wavenumbers, tumor_mean, 'r-', label='Tumor Tissue', alpha=0.7)\n", "    ax1.fill_between(wavenumbers, tumor_mean - tumor_std, tumor_mean + tumor_std, \n", "                     color='r', alpha=0.2)\n", "    ax1.set_title('Average Raman Spectra with Standard Deviation')\n", "    ax1.set_xlabel('Wavenumber (cm\u207b\u00b9)')\n", "    ax1.set_ylabel('Intensity (a.u.)')\n", "    ax1.legend()\n", "    ax1.grid(True, alpha=0.3)\n", "    \n", "    # Difference spectrum\n", "    diff_spectrum = tumor_mean - normal_mean\n", "    ax2.plot(wavenumbers, diff_spectrum, 'g-', label='Difference (Tumor - Normal)')\n", "    ax2.set_title('Difference Spectrum')\n", "    ax2.set_xlabel('Wavenumber (cm\u207b\u00b9)')\n", "    ax2.set_ylabel('Difference in Intensity')\n", "    ax2.legend()\n", "    ax2.grid(True, alpha=0.3)\n", "    \n", "    # Heatmap of all spectra\n", "    combined_spectra = np.vstack((normal_spectra, tumor_spectra))\n", "    sns.heatmap(combined_spectra, cmap='viridis', ax=ax3)\n", "    ax3.set_title('Heatmap of All Spectra')\n", "    ax3.set_xlabel('Wavenumber Index')\n", "    ax3.set_ylabel('Sample Index (Normal -> Tumor)')\n", "    \n", "    plt.tight_layout()\n", "    plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def analyze_feature_importance(X_train, y_train, X_val, y_val, wavenumbers):\n", "    \"\"\"Analyze and visualize feature importance using multiple methods\"\"\"\n", "    # Train Random Forest for feature importance\n", "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n", "    rf.fit(X_train, y_train)\n", "    rf_importance = rf.feature_importances_\n", "    \n", "    # Train XGBoost for feature importance\n", "    xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n", "    xgb_model.fit(X_train, y_train)\n", "    xgb_importance = xgb_model.feature_importances_\n", "    \n", "    # Plot feature importances\n", "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n", "    \n", "    # Random Forest importance\n", "    ax1.plot(wavenumbers, rf_importance)\n", "    ax1.set_title('Random Forest Feature Importance')\n", "    ax1.set_xlabel('Wavenumber (cm\u207b\u00b9)')\n", "    ax1.set_ylabel('Importance')\n", "    ax1.grid(True, alpha=0.3)\n", "    \n", "    # XGBoost importance\n", "    ax2.plot(wavenumbers, xgb_importance)\n", "    ax2.set_title('XGBoost Feature Importance')\n", "    ax2.set_xlabel('Wavenumber (cm\u207b\u00b9)')\n", "    ax2.set_ylabel('Importance')\n", "    ax2.grid(True, alpha=0.3)\n", "    \n", "    plt.tight_layout()\n", "    plt.show()\n", "    \n", "    # Return top important features\n", "    top_features = pd.DataFrame({\n", "        'Wavenumber': wavenumbers,\n", "        'RF_Importance': rf_importance,\n", "        'XGB_Importance': xgb_importance\n", "    }).sort_values(by='RF_Importance', ascending=False).head(10)\n", "    \n", "    return top_features"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Enhanced CNN architecture with residual connections"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class EnhancedCNN1D(nn.Module):\n", "    def __init__(self, input_size):\n", "        super(EnhancedCNN1D, self).__init__()\n", "        self.conv1 = nn.Sequential(\n", "            nn.Conv1d(1, 32, kernel_size=7, stride=2, padding=3),\n", "            nn.BatchNorm1d(32),\n", "            nn.ReLU(),\n", "            nn.MaxPool1d(2)\n", "        )\n", "        self.conv2 = nn.Sequential(\n", "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n", "            nn.BatchNorm1d(64),\n", "            nn.ReLU(),\n", "            nn.MaxPool1d(2)\n", "        )\n", "        self.conv3 = nn.Sequential(\n", "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n", "            nn.BatchNorm1d(128),\n", "            nn.ReLU(),\n", "            nn.AdaptiveMaxPool1d(8)\n", "        )\n", "        \n", "        # Residual connections\n", "        self.skip1 = nn.Conv1d(1, 32, 1)\n", "        self.skip2 = nn.Conv1d(32, 64, 1)\n", "        self.skip3 = nn.Conv1d(64, 128, 1)\n", "        \n", "        self.fc = nn.Sequential(\n", "            nn.Linear(128 * 8, 256),\n", "            nn.ReLU(),\n", "            nn.Dropout(0.5),\n", "            nn.Linear(256, 128),\n", "            nn.ReLU(),\n", "            nn.Dropout(0.3),\n", "            nn.Linear(128, 1)\n", "        )\n", "    \n", "    def forward(self, x):\n", "        # Residual connections\n", "        skip1 = self.skip1(x)\n", "        x = self.conv1(x)\n", "        x = x + skip1[:, :, :x.shape[2]]\n", "        \n", "        skip2 = self.skip2(x)\n", "        x = self.conv2(x)\n", "        x = x + skip2[:, :, :x.shape[2]]\n", "        \n", "        skip3 = self.skip3(x)\n", "        x = self.conv3(x)\n", "        x = x + skip3[:, :, :x.shape[2]]\n", "        \n", "        x = x.view(x.size(0), -1)\n", "        x = self.fc(x)\n", "        return x"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Enhanced LSTM with attention mechanism"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class AttentionLSTM1D(nn.Module):\n", "    def __init__(self, input_size):\n", "        super(AttentionLSTM1D, self).__init__()\n", "        self.lstm = nn.LSTM(input_size=1, hidden_size=128, num_layers=2,\n", "                           batch_first=True, bidirectional=True)\n", "        self.attention = nn.Sequential(\n", "            nn.Linear(256, 64),\n", "            nn.Tanh(),\n", "            nn.Linear(64, 1)\n", "        )\n", "        self.fc = nn.Sequential(\n", "            nn.Linear(256, 64),\n", "            nn.ReLU(),\n", "            nn.Dropout(0.5),\n", "            nn.Linear(64, 1)\n", "        )\n", "    \n", "    def attention_net(self, lstm_output):\n", "        attention_weights = self.attention(lstm_output)\n", "        attention_weights = torch.softmax(attention_weights, dim=1)\n", "        context = torch.sum(attention_weights * lstm_output, dim=1)\n", "        return context\n", "    \n", "    def forward(self, x):\n", "        x = x.permute(0, 2, 1)\n", "        lstm_output, _ = self.lstm(x)\n", "        attention_output = self.attention_net(lstm_output)\n", "        output = self.fc(attention_output)\n", "        return output"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def evaluate_model_enhanced(y_true, y_pred, y_prob=None):\n", "    \"\"\"Enhanced evaluation metrics\"\"\"\n", "    basic_metrics = {\n", "        'accuracy': accuracy_score(y_true, y_pred) * 100,\n", "        'precision': precision_score(y_true, y_pred) * 100,\n", "        'recall': recall_score(y_true, y_pred) * 100,\n", "        'f1_score': f1_score(y_true, y_pred) * 100\n", "    }\n", "    \n", "    # Confusion matrix metrics\n", "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n", "    basic_metrics.update({\n", "        'specificity': (tn / (tn + fp)) * 100,\n", "        'sensitivity': (tp / (tp + fn)) * 100,\n", "        'npv': (tn / (tn + fn)) * 100,  # Negative Predictive Value\n", "        'ppv': (tp / (tp + fp)) * 100   # Positive Predictive Value\n", "    })\n", "    \n", "    if y_prob is not None:\n", "        # ROC and PR curve metrics\n", "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n", "        precision, recall, _ = precision_recall_curve(y_true, y_prob)\n", "        basic_metrics.update({\n", "            'auc_roc': auc(fpr, tpr),\n", "            'auc_pr': average_precision_score(y_true, y_prob),\n", "            'roc_curve': (fpr, tpr),\n", "            'pr_curve': (precision, recall)\n", "        })\n", "    \n", "    return basic_metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_and_preprocess_data(wavenumbers_file, normal_file, tumor_part1_file, tumor_part2_file):\n", "    \"\"\"Load and preprocess the Raman spectral data from CSV files\"\"\"\n", "    # Load wavenumbers\n", "    wavenumbers = pd.read_csv(wavenumbers_file)\n", "    \n", "    # Load spectral data\n", "    normal_data = pd.read_csv(normal_file)\n", "    tumor_data1 = pd.read_csv(tumor_part1_file)\n", "    # Load tumor_data2 from the file \n", "    tumor_data2 = pd.read_csv(tumor_part2_file) # This line is changed!\n", "    \n", "    # Combine tumor data\n", "    tumor_data = pd.concat([tumor_data1, tumor_data2], axis=1) # This line is changed!\n", "    \n", "    # Convert to numpy arrays and remove any non-numeric columns\n", "    normal_spectra = normal_data.select_dtypes(include=[np.number]).values.T\n", "    tumor_spectra = tumor_data.select_dtypes(include=[np.number]).values.T\n", "    \n", "    print(\"Data shapes:\")\n", "    print(f\"Normal spectra: {normal_spectra.shape}\")\n", "    print(f\"Tumor spectra: {tumor_spectra.shape}\")\n", "    print(f\"Wavenumbers: {wavenumbers.shape}\")\n", "    \n", "    return normal_spectra, tumor_spectra, wavenumbers"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Original CNN architecture from human_1to24.py"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class HumanConv1D(nn.Module):\n", "    def __init__(self):\n", "        super(HumanConv1D, self).__init__()\n", "        \n", "        self.conv = nn.Sequential(\n", "            nn.Conv1d(in_channels=1, out_channels=32, kernel_size=5, stride=2, bias=False),\n", "            nn.BatchNorm1d(num_features=32, affine=False),\n", "            nn.ReLU(),\n", "        )\n", "        self.avgpool = nn.AvgPool1d(3, stride=1)\n", "        self.cls = nn.Sequential(nn.Linear(6976, 1, bias=False))\n\n", "        # Initialize weights\n", "        for m in self.modules():\n", "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Conv1d):\n", "                nn.init.xavier_uniform_(m.weight)\n", "                if m.bias is not None:\n", "                    m.bias.data.zero_()\n", "            elif isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.ConvTranspose1d):\n", "                nn.init.xavier_normal(m.weight)\n", "                if m.bias is not None:\n", "                    m.bias.data.zero_()\n", "            elif isinstance(m, nn.Linear):\n", "                m.weight.data.normal_(0, 0.01)\n", "                if m.bias is not None:\n", "                    m.bias.data.zero_()\n", "    def forward(self, x):\n", "        x1 = self.conv(x)\n", "        x1 = x1.view(x1.size(0), -1)\n", "        x = self.cls(x1)\n", "        return x"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Enhanced CNN architecture"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class EnhancedCNN1D(nn.Module):\n", "    def __init__(self, input_size):\n", "        super(EnhancedCNN1D, self).__init__()\n", "        self.conv1 = nn.Sequential(\n", "            nn.Conv1d(1, 32, kernel_size=7, stride=2, padding=3),\n", "            nn.BatchNorm1d(32),\n", "            nn.ReLU(),\n", "            nn.MaxPool1d(2)\n", "        )\n", "        self.conv2 = nn.Sequential(\n", "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n", "            nn.BatchNorm1d(64),\n", "            nn.ReLU(),\n", "            nn.MaxPool1d(2)\n", "        )\n", "        self.conv3 = nn.Sequential(\n", "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n", "            nn.BatchNorm1d(128),\n", "            nn.ReLU(),\n", "            nn.AdaptiveMaxPool1d(8)\n", "        )\n", "        \n", "        # Residual connections\n", "        self.skip1 = nn.Conv1d(1, 32, 1)\n", "        self.skip2 = nn.Conv1d(32, 64, 1)\n", "        self.skip3 = nn.Conv1d(64, 128, 1)\n", "        \n", "        self.fc = nn.Sequential(\n", "            nn.Linear(128 * 8, 256),\n", "            nn.ReLU(),\n", "            nn.Dropout(0.5),\n", "            nn.Linear(256, 128),\n", "            nn.ReLU(),\n", "            nn.Dropout(0.3),\n", "            nn.Linear(128, 1)\n", "        )\n", "    \n", "    def forward(self, x):\n", "        # Residual connections\n", "        skip1 = self.skip1(x)\n", "        x = self.conv1(x)\n", "        x = x + skip1[:, :, :x.shape[2]]\n", "        \n", "        skip2 = self.skip2(x)\n", "        x = self.conv2(x)\n", "        x = x + skip2[:, :, :x.shape[2]]\n", "        \n", "        skip3 = self.skip3(x)\n", "        x = self.conv3(x)\n", "        x = x + skip3[:, :, :x.shape[2]]\n", "        \n", "        x = x.view(x.size(0), -1)\n", "        x = self.fc(x)\n", "        return x"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class LSTM1D(nn.Module):\n", "    def __init__(self, input_size):\n", "        super(LSTM1D, self).__init__()\n", "        self.lstm = nn.LSTM(input_size=1, hidden_size=128, num_layers=2, \n", "                           batch_first=True, bidirectional=True)\n", "        self.fc = nn.Sequential(\n", "            nn.Linear(256, 64),\n", "            nn.ReLU(),\n", "            nn.Dropout(0.5),\n", "            nn.Linear(64, 1)\n", "        )\n", "    \n", "    def forward(self, x):\n", "        x = x.permute(0, 2, 1)\n", "        x, _ = self.lstm(x)\n", "        x = x[:, -1, :]\n", "        x = self.fc(x)\n", "        return x"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def evaluate_model(y_true, y_pred):\n", "    \"\"\"Calculate various metrics for model evaluation\"\"\"\n", "    acc = accuracy_score(y_true, y_pred)\n", "    prec = precision_score(y_true, y_pred)\n", "    rec = recall_score(y_true, y_pred)\n", "    f1 = f1_score(y_true, y_pred)\n", "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n", "    spec = tn / (tn + fp)\n", "    sens = tp / (tp + fn)\n", "    return {\n", "        'accuracy': acc * 100,\n", "        'precision': prec * 100,\n", "        'recall': rec * 100,\n", "        'f1_score': f1 * 100,\n", "        'specificity': spec * 100,\n", "        'sensitivity': sens * 100\n", "    }"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def balance_dataset(X, y, method='smote'):\n", "    \"\"\"Balance dataset using specified method\"\"\"\n", "    print(f\"\\nOriginal dataset shape: {Counter(y)}\")\n", "    \n", "    if method == 'smote':\n", "        balancer = SMOTE(random_state=42)\n", "    elif method == 'undersample':\n", "        balancer = RandomUnderSampler(random_state=42)\n", "    elif method == 'hybrid':\n", "        rus = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\n", "        X_temp, y_temp = rus.fit_resample(X, y)\n", "        balancer = SMOTE(random_state=42)\n", "        X_balanced, y_balanced = balancer.fit_resample(X_temp, y_temp)\n", "        print(f\"Balanced dataset shape: {Counter(y_balanced)}\")\n", "        return X_balanced, y_balanced\n", "    \n", "    X_balanced, y_balanced = balancer.fit_resample(X, y)\n", "    print(f\"Balanced dataset shape: {Counter(y_balanced)}\")\n", "    return X_balanced, y_balanced"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_neural_net(model, train_loader, val_loader, device, class_weights=None, epochs=70):\n", "    \"\"\"Train neural network models\"\"\"\n", "    # Move class_weights to the correct device if provided\n", "    if class_weights is not None:\n", "        class_weights = class_weights.to(device)  \n", "    \n", "    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights) if class_weights is not None else nn.BCEWithLogitsLoss()\n", "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n", "    \n", "    best_val_acc = 0\n", "    best_metrics = None\n", "    \n", "    for epoch in range(epochs):\n", "        model.train()\n", "        train_loss = 0\n", "        for batch_x, batch_y in train_loader:\n", "            # Move batch_x and batch_y to the correct device\n", "            batch_x, batch_y = batch_x.to(device), batch_y.to(device) \n", "            \n", "            optimizer.zero_grad()\n", "            outputs = model(batch_x)\n", "            loss = criterion(outputs, batch_y)\n", "            loss.backward()\n", "            optimizer.step()\n", "            train_loss += loss.item()\n", "        \n", "        # ... (rest of the function remains the same)\n", "        \n", "        # Validation\n", "        model.eval()\n", "        val_preds = []\n", "        val_true = []\n", "        with torch.no_grad():\n", "            for batch_x, batch_y in val_loader:\n", "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n", "                outputs = model(batch_x)\n", "                preds = torch.sigmoid(outputs) > 0.5\n", "                val_preds.extend(preds.cpu().numpy())\n", "                val_true.extend(batch_y.cpu().numpy())\n", "        \n", "        val_metrics = evaluate_model(val_true, val_preds)\n", "        \n", "        if val_metrics['accuracy'] > best_val_acc:\n", "            best_val_acc = val_metrics['accuracy']\n", "            best_metrics = val_metrics\n", "        \n", "        if (epoch + 1) % 10 == 0:\n", "            print(f'Epoch {epoch+1}/{epochs}:')\n", "            print(f'Train Loss: {train_loss/len(train_loader):.4f}')\n", "            print(f'Val Accuracy: {val_metrics[\"accuracy\"]:.2f}%')\n", "    \n", "    return best_metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def visualize_model_comparisons(all_results):\n", "    \"\"\"Visualize performance comparisons between models\"\"\"\n", "    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'specificity', 'sensitivity']\n", "    balance_methods = list(all_results.keys())\n", "    models = list(all_results[balance_methods[0]].keys())\n", "    \n", "    # Create subplots for each metric\n", "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n", "    axes = axes.flatten()\n", "    \n", "    for idx, metric in enumerate(metrics):\n", "        ax = axes[idx]\n", "        \n", "        # Prepare data for plotting\n", "        data = []\n", "        labels = []\n", "        for method in balance_methods:\n", "            metric_values = [all_results[method][model][metric] \n", "                           for model in models if metric in all_results[method][model]]\n", "            data.append(metric_values)\n", "            labels.extend([f\"{model}\\n({method})\" for model in models])\n", "        \n", "        # Create boxplot\n", "        bp = ax.boxplot(data, labels=balance_methods, patch_artist=True)\n", "        \n", "        # Customize colors\n", "        colors = ['lightblue', 'lightgreen', 'lightpink']\n", "        for patch, color in zip(bp['boxes'], colors):\n", "            patch.set_facecolor(color)\n", "        \n", "        ax.set_title(f'{metric.capitalize()} Comparison')\n", "        ax.set_ylabel('Percentage')\n", "        ax.grid(True, alpha=0.3)\n", "        ax.set_xticklabels(balance_methods, rotation=45)\n", "    \n", "    plt.tight_layout()\n", "    plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def visualize_learning_curves(model, train_loader, val_loader, device, epochs=70):\n", "    \"\"\"Visualize learning curves during training\"\"\"\n", "    train_losses = []\n", "    train_accs = []\n", "    val_accs = []\n", "    val_losses = []\n", "    \n", "    criterion = nn.BCEWithLogitsLoss()\n", "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n", "    \n", "    for epoch in range(epochs):\n", "        # Training\n", "        model.train()\n", "        epoch_train_loss = 0\n", "        epoch_train_correct = 0\n", "        epoch_train_total = 0\n", "        \n", "        for batch_x, batch_y in train_loader:\n", "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n", "            \n", "            optimizer.zero_grad()\n", "            outputs = model(batch_x)\n", "            loss = criterion(outputs, batch_y)\n", "            loss.backward()\n", "            optimizer.step()\n", "            \n", "            epoch_train_loss += loss.item()\n", "            preds = torch.sigmoid(outputs) > 0.5\n", "            epoch_train_correct += (preds == batch_y).sum().item()\n", "            epoch_train_total += batch_y.size(0)\n", "        \n", "        # Validation\n", "        model.eval()\n", "        epoch_val_loss = 0\n", "        epoch_val_correct = 0\n", "        epoch_val_total = 0\n", "        \n", "        with torch.no_grad():\n", "            for batch_x, batch_y in val_loader:\n", "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n", "                outputs = model(batch_x)\n", "                loss = criterion(outputs, batch_y)\n", "                \n", "                epoch_val_loss += loss.item()\n", "                preds = torch.sigmoid(outputs) > 0.5\n", "                epoch_val_correct += (preds == batch_y).sum().item()\n", "                epoch_val_total += batch_y.size(0)\n", "        \n", "        # Record metrics\n", "        train_losses.append(epoch_train_loss / len(train_loader))\n", "        train_accs.append(100 * epoch_train_correct / epoch_train_total)\n", "        val_losses.append(epoch_val_loss / len(val_loader))\n", "        val_accs.append(100 * epoch_val_correct / epoch_val_total)\n", "        \n", "        if (epoch + 1) % 10 == 0:\n", "            print(f'Epoch {epoch+1}/{epochs}:')\n", "            print(f'Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accs[-1]:.2f}%')\n", "            print(f'Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accs[-1]:.2f}%')\n", "    \n", "    # Plot learning curves\n", "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n", "    \n", "    # Loss curves\n", "    ax1.plot(train_losses, label='Train Loss')\n", "    ax1.plot(val_losses, label='Validation Loss')\n", "    ax1.set_title('Loss Curves')\n", "    ax1.set_xlabel('Epoch')\n", "    ax1.set_ylabel('Loss')\n", "    ax1.legend()\n", "    ax1.grid(True, alpha=0.3)\n", "    \n", "    # Accuracy curves\n", "    ax2.plot(train_accs, label='Train Accuracy')\n", "    ax2.plot(val_accs, label='Validation Accuracy')\n", "    ax2.set_title('Accuracy Curves')\n", "    ax2.set_xlabel('Epoch')\n", "    ax2.set_ylabel('Accuracy (%)')\n", "    ax2.legend()\n", "    ax2.grid(True, alpha=0.3)\n", "    \n", "    plt.tight_layout()\n", "    plt.show()\n", "    \n", "    return train_losses, train_accs, val_losses, val_accs"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def analyze_architectures():\n", "    \"\"\"Analyze and compare CNN architectures\"\"\"\n", "    # Original CNN Architecture Analysis\n", "    original_params = sum(p.numel() for p in HumanConv1D().parameters())\n", "    \n", "    print(\"Original CNN Architecture Analysis:\")\n", "    print(\"-\" * 50)\n", "    print(\"Layer Structure:\")\n", "    print(\"1. Input Layer: 1 channel\")\n", "    print(\"2. Convolutional Layer:\")\n", "    print(\"   - Input channels: 1\")\n", "    print(\"   - Output channels: 32\")\n", "    print(\"   - Kernel size: 5\")\n", "    print(\"   - Stride: 2\")\n", "    print(\"   - No bias\")\n", "    print(\"3. Batch Normalization (non-affine)\")\n", "    print(\"4. ReLU Activation\")\n", "    print(\"5. Average Pooling:\")\n", "    print(\"   - Kernel size: 3\")\n", "    print(\"   - Stride: 1\")\n", "    print(\"6. Fully Connected Layer:\")\n", "    print(\"   - Input: 6976\")\n", "    print(\"   - Output: 1\")\n", "    print(f\"Total Parameters: {original_params:,}\")\n", "    print(\"\\nKey Characteristics:\")\n", "    print(\"- Simple architecture with single conv layer\")\n", "    print(\"- No dropout (relies on batch norm for regularization)\")\n", "    print(\"- Xavier weight initialization\")\n", "    print(\"- Average pooling instead of max pooling\")\n", "    \n", "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n", "    \n", "    # Enhanced CNN Architecture Analysis\n", "    enhanced_params = sum(p.numel() for p in EnhancedCNN1D(440).parameters())\n", "    \n", "    print(\"Enhanced CNN Architecture Analysis:\")\n", "    print(\"-\" * 50)\n", "    print(\"Layer Structure:\")\n", "    print(\"1. Input Layer: 1 channel\")\n", "    print(\"2. First Convolutional Block:\")\n", "    print(\"   - Conv1d: 1 \u2192 32 channels, kernel=7, stride=2\")\n", "    print(\"   - Batch Normalization\")\n", "    print(\"   - ReLU\")\n", "    print(\"   - MaxPool1d\")\n", "    print(\"3. Second Convolutional Block:\")\n", "    print(\"   - Conv1d: 32 \u2192 64 channels, kernel=5\")\n", "    print(\"   - Batch Normalization\")\n", "    print(\"   - ReLU\")\n", "    print(\"   - MaxPool1d\")\n", "    print(\"4. Third Convolutional Block:\")\n", "    print(\"   - Conv1d: 64 \u2192 128 channels, kernel=3\")\n", "    print(\"   - Batch Normalization\")\n", "    print(\"   - ReLU\")\n", "    print(\"   - AdaptiveMaxPool1d\")\n", "    print(\"5. Residual Connections:\")\n", "    print(\"   - Skip connections between conv blocks\")\n", "    print(\"6. Fully Connected Layers:\")\n", "    print(\"   - 128*8 \u2192 256 \u2192 128 \u2192 1\")\n", "    print(\"   - Dropout layers (0.5, 0.3)\")\n", "    print(f\"Total Parameters: {enhanced_params:,}\")\n", "    print(\"\\nKey Characteristics:\")\n", "    print(\"- Deeper architecture with 3 conv layers\")\n", "    print(\"- Residual connections for better gradient flow\")\n", "    print(\"- Multiple dropout layers for regularization\")\n", "    print(\"- Adaptive pooling for input size flexibility\")\n", "    print(\"- Gradually increasing channel depth\")\n", "    \n", "    # Architecture Comparison\n", "    print(\"\\nArchitecture Comparison:\")\n", "    print(\"-\" * 50)\n", "    print(f\"Parameter Ratio (Enhanced/Original): {enhanced_params/original_params:.2f}x\")\n", "    print(\"\\nKey Differences:\")\n", "    print(\"1. Depth: Enhanced is deeper with 3 conv layers vs 1\")\n", "    print(\"2. Regularization: Enhanced uses dropout, Original uses only batch norm\")\n", "    print(\"3. Pooling: Enhanced uses max pooling, Original uses average pooling\")\n", "    print(\"4. Skip Connections: Enhanced has residual connections, Original is linear\")\n", "    print(\"5. FC Layers: Enhanced has multiple FC layers with dropout\")\n", "    print(\"6. Parameter Count: Enhanced has more parameters for better feature extraction\")\n", "def visualize_holdout_comparison(train_results, val_results, test_results, model_name):\n", "    \"\"\"Visualize performance comparison across train, validation, and test sets\"\"\"\n", "    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'specificity', 'sensitivity']\n", "    \n", "    plt.figure(figsize=(12, 6))\n", "    x = np.arange(len(metrics))\n", "    width = 0.25\n", "    \n", "    plt.bar(x - width, [train_results[m] for m in metrics], width, label='Train')\n", "    plt.bar(x, [val_results[m] for m in metrics], width, label='Validation')\n", "    plt.bar(x + width, [test_results[m] for m in metrics], width, label='Test')\n", "    \n", "    plt.xlabel('Metrics')\n", "    plt.ylabel('Performance (%)')\n", "    plt.title(f'Performance Comparison - {model_name}')\n", "    plt.xticks(x, metrics, rotation=45)\n", "    plt.legend()\n", "    plt.grid(True, alpha=0.3)\n", "    plt.tight_layout()\n", "    plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_neural_net_holdout(model, train_loader, val_loader, test_loader, device, \n", "                           class_weights=None, epochs=70):\n", "    \"\"\"Train neural network with holdout evaluation\"\"\"\n", "    if class_weights is not None:\n", "        class_weights = class_weights.to(device)\n", "    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights) if class_weights is not None else nn.BCEWithLogitsLoss()\n", "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n", "    \n", "    best_val_acc = 0\n", "    best_val_metrics = None\n", "    best_test_metrics = None\n", "    best_train_metrics = None\n", "    best_model_state = None\n", "    \n", "    train_losses = []\n", "    val_losses = []\n", "    test_losses = []\n", "    \n", "    for epoch in range(epochs):\n", "        # Training phase\n", "        model.train()\n", "        train_loss = 0\n", "        train_preds = []\n", "        train_true = []\n", "        \n", "        for batch_x, batch_y in train_loader:\n", "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n", "            \n", "            optimizer.zero_grad()\n", "            outputs = model(batch_x)\n", "            loss = criterion(outputs, batch_y)\n", "            loss.backward()\n", "            optimizer.step()\n", "            \n", "            train_loss += loss.item()\n", "            preds = torch.sigmoid(outputs) > 0.5\n", "            train_preds.extend(preds.cpu().numpy())\n", "            train_true.extend(batch_y.cpu().numpy())\n", "        \n", "        # Validation phase\n", "        model.eval()\n", "        val_loss = 0\n", "        val_preds = []\n", "        val_true = []\n", "        val_probs = []\n", "        \n", "        with torch.no_grad():\n", "            for batch_x, batch_y in val_loader:\n", "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n", "                outputs = model(batch_x)\n", "                loss = criterion(outputs, batch_y)\n", "                val_loss += loss.item()\n", "                \n", "                probs = torch.sigmoid(outputs)\n", "                preds = probs > 0.5\n", "                val_preds.extend(preds.cpu().numpy())\n", "                val_true.extend(batch_y.cpu().numpy())\n", "                val_probs.extend(probs.cpu().numpy())\n", "        \n", "        train_metrics = evaluate_model_enhanced(train_true, train_preds)\n", "        val_metrics = evaluate_model_enhanced(val_true, val_preds, val_probs)\n", "        \n", "        train_losses.append(train_loss / len(train_loader))\n", "        val_losses.append(val_loss / len(val_loader))\n", "        \n", "        # Evaluate on test set if validation accuracy improves\n", "        if val_metrics['accuracy'] > best_val_acc:\n", "            best_val_acc = val_metrics['accuracy']\n", "            best_val_metrics = val_metrics\n", "            best_train_metrics = train_metrics\n", "            best_model_state = model.state_dict().copy()\n", "            \n", "            # Test set evaluation\n", "            test_preds = []\n", "            test_true = []\n", "            test_probs = []\n", "            test_loss = 0\n", "            \n", "            with torch.no_grad():\n", "                for batch_x, batch_y in test_loader:\n", "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n", "                    outputs = model(batch_x)\n", "                    loss = criterion(outputs, batch_y)\n", "                    test_loss += loss.item()\n", "                    \n", "                    probs = torch.sigmoid(outputs)\n", "                    preds = probs > 0.5\n", "                    test_preds.extend(preds.cpu().numpy())\n", "                    test_true.extend(batch_y.cpu().numpy())\n", "                    test_probs.extend(probs.cpu().numpy())\n", "            \n", "            best_test_metrics = evaluate_model_enhanced(test_true, test_preds, test_probs)\n", "            test_losses.append(test_loss / len(test_loader))\n", "        \n", "        if (epoch + 1) % 10 == 0:\n", "            print(f'Epoch {epoch+1}/{epochs}:')\n", "            print(f'Train Loss: {train_loss/len(train_loader):.4f}')\n", "            print(f'Val Loss: {val_loss/len(val_loader):.4f}')\n", "            print(f'Train Accuracy: {train_metrics[\"accuracy\"]:.2f}%')\n", "            print(f'Val Accuracy: {val_metrics[\"accuracy\"]:.2f}%')\n", "    \n", "    # Plot learning curves\n", "    plt.figure(figsize=(10, 5))\n", "    plt.plot(train_losses, label='Train Loss')\n", "    plt.plot(val_losses, label='Validation Loss')\n", "    if test_losses:\n", "        plt.plot(test_losses, label='Test Loss')\n", "    plt.title('Learning Curves')\n", "    plt.xlabel('Epoch')\n", "    plt.ylabel('Loss')\n", "    plt.legend()\n", "    plt.grid(True, alpha=0.3)\n", "    plt.show()\n", "    \n", "    # Restore best model\n", "    model.load_state_dict(best_model_state)\n", "    return best_train_metrics, best_val_metrics, best_test_metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def compare_methods_holdout(normal_spectra, tumor_spectra, test_size=0.1, val_size=0.15, \n", "                          balance_method='smote'):\n", "    \"\"\"Compare different classification methods with smaller holdout testing\n", "    \n", "    Parameters:\n", "    -----------\n", "    test_size : float, default=0.1\n", "        Percentage of data to use for final testing (10%)\n", "    val_size : float, default=0.15\n", "        Percentage of remaining data to use for validation (15% of 90%)\n", "    \"\"\"\n", "    # Combine data\n", "    X = np.concatenate((normal_spectra, tumor_spectra), axis=0)\n", "    y = np.concatenate((np.zeros(len(normal_spectra)), np.ones(len(tumor_spectra))))\n", "    \n", "    # First split: separate holdout test set (10%)\n", "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=test_size, \n", "                                                     random_state=42, stratify=y)\n", "    \n", "    # Second split: separate training and validation sets (15% of remaining data)\n", "    val_size_adj = val_size / (1 - test_size)  # Adjust validation size\n", "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, \n", "                                                     test_size=val_size_adj, \n", "                                                     random_state=42, stratify=y_temp)\n", "    \n", "    # Print split sizes for verification\n", "    print(f\"\\nDataset splits:\")\n", "    print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n", "    print(f\"Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n", "    print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n", "    \n", "    # Balance training data\n", "    X_train_balanced, y_train_balanced = balance_dataset(X_train, y_train, method=balance_method)\n", "    \n", "    # Visualize data\n", "    visualize_spectra(normal_spectra, tumor_spectra, np.arange(X.shape[1]))\n", "    \n", "    # Standardize features\n", "    scaler = StandardScaler()\n", "    X_train_scaled = scaler.fit_transform(X_train_balanced)\n", "    X_val_scaled = scaler.transform(X_val)\n", "    X_test_scaled = scaler.transform(X_test)\n", "    \n", "    # Analyze feature importance\n", "    importance_results = analyze_feature_importance(X_train_scaled, y_train_balanced, \n", "                                                 X_val_scaled, y_val, \n", "                                                 np.arange(X.shape[1]))\n", "    print(\"\\nTop Important Features:\")\n", "    print(importance_results)\n", "    \n", "    print(f\"\\nDataset splits:\")\n", "    print(f\"Training set: {X_train_scaled.shape[0]} samples\")\n", "    print(f\"Validation set: {X_val_scaled.shape[0]} samples\")\n", "    print(f\"Test set: {X_test_scaled.shape[0]} samples\")\n", "    \n", "    # Calculate class weights\n", "    class_weights = torch.tensor([len(y_train[y_train == 0]) / len(y_train[y_train == 1])])\n", "    \n", "    # Prepare PyTorch data\n", "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n", "    print(f\"\\nUsing device: {device}\")\n", "    \n", "    # Create datasets and loaders\n", "    train_dataset = TensorDataset(\n", "        torch.FloatTensor(X_train_scaled).unsqueeze(1),\n", "        torch.FloatTensor(y_train_balanced).unsqueeze(1)\n", "    )\n", "    val_dataset = TensorDataset(\n", "        torch.FloatTensor(X_val_scaled).unsqueeze(1),\n", "        torch.FloatTensor(y_val).unsqueeze(1)\n", "    )\n", "    test_dataset = TensorDataset(\n", "        torch.FloatTensor(X_test_scaled).unsqueeze(1),\n", "        torch.FloatTensor(y_test).unsqueeze(1)\n", "    )\n", "    \n", "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n", "    val_loader = DataLoader(val_dataset, batch_size=32)\n", "    test_loader = DataLoader(test_dataset, batch_size=32)\n", "    \n", "    results = {}\n", "    \n", "    # Train and evaluate traditional ML models\n", "    models = {\n", "        'SVM': SVC(kernel='rbf', class_weight='balanced', probability=True),\n", "        'Random Forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),\n", "        'XGBoost': xgb.XGBClassifier(scale_pos_weight=len(y_train[y_train == 0]) / len(y_train[y_train == 1]),\n", "                                    use_label_encoder=False, eval_metric='logloss'),\n", "        'MLP': MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=1000)\n", "    }\n", "    \n", "    for name, model in models.items():\n", "        print(f\"\\nTraining {name}...\")\n", "        model.fit(X_train_scaled, y_train_balanced)\n", "        \n", "        # Get predictions and probabilities for all sets\n", "        train_preds = model.predict(X_train_scaled)\n", "        val_preds = model.predict(X_val_scaled)\n", "        test_preds = model.predict(X_test_scaled)\n", "        \n", "        train_probs = model.predict_proba(X_train_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n", "        val_probs = model.predict_proba(X_val_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n", "        test_probs = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n", "        \n", "        # Evaluate on all sets\n", "        train_metrics = evaluate_model_enhanced(y_train_balanced, train_preds, train_probs)\n", "        val_metrics = evaluate_model_enhanced(y_val, val_preds, val_probs)\n", "        test_metrics = evaluate_model_enhanced(y_test, test_preds, test_probs)\n", "        \n", "        results[name] = {\n", "            'train': train_metrics,\n", "            'validation': val_metrics,\n", "            'test': test_metrics\n", "        }\n", "        \n", "        # Visualize performance comparison\n", "        visualize_holdout_comparison(train_metrics, val_metrics, test_metrics, name)\n", "    \n", "    # Analyze and train neural networks\n", "    analyze_architectures()\n", "    \n", "    print(\"\\nTraining Original 1D CNN...\")\n", "    original_cnn = HumanConv1D().to(device)\n", "    original_train, original_val, original_test = train_neural_net_holdout(\n", "        original_cnn, train_loader, val_loader, test_loader, device, class_weights\n", "    )\n", "    results['Original CNN'] = {\n", "        'train': original_train,\n", "        'validation': original_val,\n", "        'test': original_test\n", "    }\n", "    visualize_holdout_comparison(original_train, original_val, original_test, 'Original CNN')\n", "    \n", "    print(\"\\nTraining Enhanced 1D CNN...\")\n", "    enhanced_cnn = EnhancedCNN1D(X_train_scaled.shape[1]).to(device)\n", "    enhanced_train, enhanced_val, enhanced_test = train_neural_net_holdout(\n", "        enhanced_cnn, train_loader, val_loader, test_loader, device, class_weights\n", "    )\n", "    results['Enhanced CNN'] = {\n", "        'train': enhanced_train,\n", "        'validation': enhanced_val,\n", "        'test': enhanced_test\n", "    }\n", "    visualize_holdout_comparison(enhanced_train, enhanced_val, enhanced_test, 'Enhanced CNN')\n", "    \n", "    return results"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == '__main__':\n", "    # File paths\n", "    wavenumbers_file = 'x_axis_wavenumbers 1.xlsx.csv'\n", "    normal_file = 'y_normalize_data_NormalTissue_update (2).xlsx.csv'\n", "    tumor_part1_file = 'y_normalize_data_Tumor_Part1_update (2).xlsx.csv'\n", "    tumor_part2_file = 'y_normalize_data_Tumor_Part2_update.xlsx.csv'\n", "    \n", "    # Load and preprocess data\n", "    normal_spectra, tumor_spectra, wavenumbers = load_and_preprocess_data(\n", "        wavenumbers_file, normal_file, tumor_part1_file, tumor_part2_file\n", "    )\n", "    \n", "    # Run comparison with different balancing methods\n", "    balance_methods = ['smote', 'undersample', 'hybrid']\n", "    all_results = {}\n", "    \n", "    for method in balance_methods:\n", "        print(f\"\\nRunning comparison with {method.upper()} balancing:\")\n", "        results = compare_methods_holdout(normal_spectra, tumor_spectra, \n", "                                        test_size=0.2, val_size=0.2,\n", "                                        balance_method=method)\n", "        all_results[method] = results\n", "    \n", "    # Print comprehensive results\n", "    print(\"\\nFinal Results Summary:\")\n", "    print(\"=\" * 120)\n", "    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'specificity', 'sensitivity', 'auc_roc']\n", "    sets = ['train', 'validation', 'test']\n", "    \n", "    for balance_method, results in all_results.items():\n", "        print(f\"\\n{balance_method.upper()} Balancing Results:\")\n", "        print(\"-\" * 120)\n", "        \n", "        for set_name in sets:\n", "            print(f\"\\n{set_name.upper()} Set Results:\")\n", "            print(\"-\" * 100)\n", "            \n", "            # Print header\n", "            print(f\"{'Method':<15}\", end=\"\")\n", "            for metric in metrics:\n", "                print(f\"{metric:<12}\", end=\"\")\n", "            print(\"\\n\" + \"-\" * 100)\n", "            \n", "            # Print results for each model\n", "            for method, scores in results.items():\n", "                print(f\"{method:<15}\", end=\"\")\n", "                for metric in metrics:\n", "                    if metric in scores[set_name]:\n", "                        print(f\"{scores[set_name][metric]:>11.2f}\", end=\" \")\n", "                    else:\n", "                        print(f\"{'N/A':>11}\", end=\" \")\n", "                print()\n", "    \n", "    # Plot comparative analysis\n", "    plt.figure(figsize=(15, 10))\n", "    \n", "    # Create subplots for each metric comparing train/val/test performance\n", "    plot_metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'specificity', 'sensitivity']  # Remove auc_roc from plotting\n", "    \n", "    for idx, metric in enumerate(plot_metrics):\n", "        plt.subplot(2, 3, idx + 1)\n", "        \n", "        x_pos = np.arange(len(results.keys()))\n", "        width = 0.25\n", "        \n", "        for method_idx, balance_method in enumerate(balance_methods):\n", "            try:\n", "                train_scores = []\n", "                val_scores = []\n", "                test_scores = []\n", "                \n", "                for model in results.keys():\n", "                    # Safely get scores with error handling\n", "                    try:\n", "                        train_scores.append(all_results[balance_method][model]['train'][metric])\n", "                        val_scores.append(all_results[balance_method][model]['validation'][metric])\n", "                        test_scores.append(all_results[balance_method][model]['test'][metric])\n", "                    except (KeyError, TypeError):\n", "                        train_scores.append(np.nan)\n", "                        val_scores.append(np.nan)\n", "                        test_scores.append(np.nan)\n", "                \n", "                # Plot only if we have valid scores\n", "                if any(not np.isnan(score) for score in train_scores):\n", "                    plt.bar(x_pos + method_idx*width - width, train_scores, width, \n", "                           label=f'{balance_method}-train' if idx == 0 else \"\", alpha=0.7)\n", "                if any(not np.isnan(score) for score in val_scores):\n", "                    plt.bar(x_pos + method_idx*width, val_scores, width, \n", "                           label=f'{balance_method}-val' if idx == 0 else \"\", alpha=0.7)\n", "                if any(not np.isnan(score) for score in test_scores):\n", "                    plt.bar(x_pos + method_idx*width + width, test_scores, width, \n", "                           label=f'{balance_method}-test' if idx == 0 else \"\", alpha=0.7)\n", "                    \n", "            except Exception as e:\n", "                print(f\"Error plotting {metric} for {balance_method}: {str(e)}\")\n", "                continue\n", "        \n", "        plt.title(f'{metric}')\n", "        plt.xticks(x_pos + width, list(results.keys()), rotation=45)\n", "        if idx == 0:\n", "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n", "        plt.grid(True, alpha=0.3)\n", "        plt.ylim(0, 100)  # Set y-axis limit for percentage metrics\n", "    \n", "    plt.tight_layout()\n", "    plt.show()\n", "    \n", "    # Save results to file\n", "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n", "    results_file = f'raman_classification_results_{timestamp}.txt'\n", "    \n", "    with open(results_file, 'w') as f:\n", "        f.write(\"Raman Classification Results\\n\")\n", "        f.write(\"=\" * 100 + \"\\n\\n\")\n", "        \n", "        for balance_method, results in all_results.items():\n", "            f.write(f\"\\n{balance_method.upper()} Balancing Results:\\n\")\n", "            f.write(\"-\" * 100 + \"\\n\")\n", "            \n", "            for set_name in sets:\n", "                f.write(f\"\\n{set_name.upper()} Set Results:\\n\")\n", "                f.write(\"-\" * 80 + \"\\n\")\n", "                \n", "                # Write header\n", "                f.write(f\"{'Method':<15}\")\n", "                for metric in plot_metrics:  # Use plot_metrics instead of metrics\n", "                    f.write(f\"{metric:<12}\")\n", "                f.write(\"\\n\" + \"-\" * 80 + \"\\n\")\n", "                \n", "                # Write results for each model\n", "                for method, model_results in results.items():\n", "                    f.write(f\"{method:<15}\")\n", "                    for metric in plot_metrics:  # Use plot_metrics instead of metrics\n", "                        try:\n", "                            if metric in model_results[set_name]:\n", "                                f.write(f\"{model_results[set_name][metric]:>11.2f} \")\n", "                            else:\n", "                                f.write(f\"{'N/A':>11} \")\n", "                        except (KeyError, TypeError):\n", "                            f.write(f\"{'N/A':>11} \")\n", "                    f.write(\"\\n\")\n", "                \n", "                f.write(\"\\n\")\n", "    \n", "    print(f\"\\nResults have been saved to {results_file}\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}